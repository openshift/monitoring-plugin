## 3. CRITICAL: Data Loading â€“ API Call Bugs

**Automation Status**: PARTIALLY AUTOMATED (Sections 3.1 and 3.2)

### Prerequisites: Test Data Setup for Data Loading Tests

**CSV Format** - These alerts test resolution, short duration, and silence logic (creates incidents F, G, I, J):

```csv
start,end,alertname,namespace,severity,silenced,labels
1140,1260,AlertF_KubePodCrashLooping,openshift-monitoring,warning,false,{"component": "monitoring"}
1200,1380,AlertF_HighMemoryUsage,openshift-monitoring,critical,false,{"component": "monitoring"}
1440,1500,AlertG_APIServerLatency,openshift-kube-apiserver,warning,false,{"component": "kube-apiserver"}
1800,1980,AlertI_KubePodNotReady,openshift-operators,warning,true,{"component": "operators"}
2040,2220,AlertJ_KubePodNotReady,openshift-storage,warning,false,{"component": "storage"}
```

**Silence Matching Logic**:
- Silence determined by `silenced` field in CSV (becomes label in `cluster_health_components_map` metric)
- Incidents I and J have same `alertname` but different `namespace` and different `silenced` values
- Tests that silence matching uses: `alertname` + `namespace` + `severity` (NOT just alert name)

**Quick Reference** (Alerts & Silences):
| Alert | alertname | namespace | severity | Time Range | Expected State | silenced | Use For Testing |
|-------|-----------|-----------|----------|------------|----------------|----------|-----------------|
| F1 | AlertF_KubePodCrashLooping | openshift-monitoring | warning | 1140-1260 | Resolved | false | Time-based resolution |
| F2 | AlertF_HighMemoryUsage | openshift-monitoring | critical | 1200-1380 | Resolved | false | Time-based resolution |
| G | AlertG_APIServerLatency | openshift-kube-apiserver | warning | 1440-1500 | Resolved | false | Short duration (1 hr) |
| I | AlertI_KubePodNotReady | openshift-operators | warning | 1800-1980 | Resolved | **true** | Silence matching |
| J | AlertJ_KubePodNotReady | openshift-storage | warning | 2040-2220 | Resolved | **false** | Different namespace = not silenced |


### 3.1 Short Incidents Not Visible
**BUG**: Incidents with duration < 5 minutes weren't showing up.  
**Automation Status**: AUTOMATED in `02.reg_ui_charts_comprehensive.cy.ts` (Section 3.1)
- Uses fixture: `incident-scenarios/12-charts-ui-comprehensive.yaml` (includes very short duration incidents)
- Tests: 5-minute, 9-minute, and recently resolved (2 min ago) incidents
- Verifies: Bar visibility, dimensions, transparency, selectability, and alert loading

- [x] **Short Incident C**: Check `api-server` incident (0 min duration, single point) - AUTOMATED
  - Verify appears in incidents chart (has visible bar despite 0 duration)
  - Select it and verify alert loads
  
- [x] **Short Incident G**: Check `kube-apiserver` incident (60 min duration) - AUTOMATED
  - Verify appears in incidents chart with visible bar
  - Select it and verify AlertG_APIServerLatency appears
  - Verify no minimum duration threshold filters it out

### 3.2 Silences Not Applied Correctly
**BUG**: Silences were being matched by name only, not by name + namespace + severity.  
**Automation Status**: AUTOMATED in `03.reg_api_calls.cy.ts`
- Uses fixture: `incident-scenarios/9-silenced-alerts-mixed-scenario.yaml`
- Verifies: Opacity (0.3 for silenced, 1.0 for non-silenced)
- Verifies: Tooltip "(silenced)" indicator
- Tests: Same alert name with different namespaces

- [ ] **Incident I IS Silenced**: Check `AlertI_KubePodNotReady` in `openshift-operators`
  - Silence matches: name=`KubePodNotReady` + namespace=`openshift-operators` + severity=`warning`
  - Expected: Alert marked as `silenced = true`
  - Verify alert bar has opacity: 0.3 (reduced)
  - Verify tooltip shows: "AlertI_KubePodNotReady (silenced)"
  
- [ ] **Incident J NOT Silenced**: Check `AlertJ_KubePodNotReady` in `openshift-storage`
  - Same alert name as Incident I, but DIFFERENT namespace
  - Silence does NOT match (namespace mismatch)
  - Expected: Alert marked as `silenced = false`
  - Verify alert bar has opacity: 1.0 (full)
  - Verify tooltip shows: "AlertJ_KubePodNotReady" (no silenced suffix)
  
- [ ] **Silence Matching Logic**: Verify implementation
  - Check that matching uses: `alertname` + `namespace` + `severity`
  - NOT just `alertname` alone
  - Silence source: `cluster_health_components_map` metric (NOT Alertmanager API)

  ### 3.3 Alerts Marked as Resolved After Time
**BUG**: Alerts not being marked as resolved when they should be.  
**Automation Status**: NOT AUTOMATED (requires live firing alerts)
- **WARNING Not possible to test on Injected Data, requires continously firing alert**
  - Trigger a real firing alert (Pod CrashLooping...)
  - Verify that the alert is firing
  - Wait for 10 minutes without refreshing incidents
  - Toggle the days filter to retrigger the alert queries
  - Verify it is not marked as resolved
  - Verify the latest query end time param is within the last 5 minutes


### 3.4 15-Day Data Loading with "Last N Days" Filtering
**FEATURE**: UI always loads 15 days of data (one query_range call per day), then filters client-side based on "Last N Days" selection.  
**Automation Status**: NOT AUTOMATED

**Background**:
- Before: Data was downloaded only for "Last N Days", causing Start dates to be relative to N days
- After: Start displays an absolute date, even when "Last N Days" is shorter than the incident's actual start
- Limit: Start is capped at max 15 days (the maximum supported range)

**Fix Implementation**:
The absolute start date of an incident/alert is always displayed, regardless of the selected "Last N Days" filter.

Solution uses a new API call:
- Absolute timestamps are retrieved by performing an **instant query** call to Prometheus
- For incidents: `min_over_time(timestamp(cluster_health_components_map{}))`
- For alerts: `min_over_time(timestamp(ALERTS{}))`
- This returns the timestamp of the first datapoint for that metric
- The result is saved into Redux store and matched to related incident/alert to update the Start date displayed in the tooltip

**Manual Testing Data**:
Use `docs/incident_detection/simulate_scenarios/long-incident-15-days.csv` which creates a 15-day spanning incident for testing absolute start date display.

- [ ] **Absolute Start Date Display**: Use `long-incident-15-days.csv` (creates 15-day incident)
  - Set time filter to "Last 7 days"
  - Verify incident Start date shows the absolute date (15 days ago), NOT a date relative to 7 days
  - Verify the incident bar in the chart is trimmed to show only the portion within the 7-day window
  - Verify tooltip shows the actual absolute start time

- [ ] **API Call Pattern Verification**: Monitor network requests on initial page load
  - Verify 15 query_range calls are made on initial page load (one per day)
  - Verify instant query calls for `min_over_time(timestamp(cluster_health_components_map{}))` and `min_over_time(timestamp(ALERTS{}))`
  - Verify the time ranges cover the full 15-day window regardless of "Last N Days" selection

### 3.5 Data Integrity
**NEW, NOT AUTOMATED, TODO COO 1.4**
- [ ] Incident grouping by `group_id` works correctly
- [ ] Values deduplicated across multiple time range queries
- [ ] Component lists combined for same group_id
- [ ] Watchdog alerts filtered out

